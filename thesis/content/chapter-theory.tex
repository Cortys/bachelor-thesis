% !TEX root = ../main.tex
% chktex-file 26
% chktex-file 36
% chktex-file 46
\def\cgCutScale{0.23}

\chapter{Grundlagen und Hilfsmittel}%
\label{sec:theory}

In Kapitel~\ref{sec:related} wurde ein Überblick über das Problemumfeld der Wissensgraph-Konstruk\-tion gegeben.
Diese Arbeit baut insbesondere auf den bereits kurz vorgestellten Konzeptgraphen, Stanfords~CoreNLP Bibliothek und der PSL auf.
Für die folgenden Kapitel ist ein Grundverständnis dieser drei Hilfsmittel notwendig.
Sie werden daher in den folgenden Abschnitten näher beschrieben.

\section{Wissensmodellierung mit Konzeptgraphen}%
\label{sec:theory:cg}

John F. Sowas Konzeptgraphen~\cite{Harmelen2007} bilden die Basis der Graphontologie dieser Arbeit.
Wie in~\ref{sec:related:kr:history} beschrieben, sind sie ein auf Existenzgraphen basierendes logisches Kalkül.
Die vollständige Konzeptgraphsyntax geht allerdings weit über die Prädikatenlogik hinaus, da auch Modallogik und natürlichsprachliche Konzepte, wie z.~B. Fragen und Betonungen, unterstützt werden.
Da Sowas eigene Beschreibungen diesbezüglich teils etwas unklar sind~\cite{Wermelinger1995}, werden im Folgenden lediglich die sog.~\textit{Conceptual Graphs with Cuts}~\cite{Dau2003} vorgestellt.
Sie sind eine zur Prädikatenlogik erster Ordnung äquivalente, formal spezifizierte Teilmenge der Konzeptgraphen, die ein Kalkül bildet, dessen Vollständigkeit und Korrektheit bewiesen ist.

\subsection{Syntax}%
\label{sec:theory:cg:syntax}

In ihrer einfachsten Form lassen sich Konzeptgraphen als Graphen mit drei Arten von Knoten und zwei Arten von Kanten beschreiben.

\paragraph{Konzeptknoten (\textit{concepts})}
Entsprechen in etwa existenzquantisiert gebundenen Variablen.
Wie auch in der Prädikatenlogik, haben die Bezeichner von Konzeptknoten keine semantische Relevanz und können frei gewählt werden.
\begin{align*}
	\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/conceptNode.pdf}}}
	\quad\Leftrightarrow\quad
	{\color{rot}\exists\ a, b} \numberthis
\end{align*}

\paragraph{Relationsknoten (\textit{conceptual relations}) und Argumentkanten (\textit{arguments})}
Relationsknoten entsprechen prädikatenlogischen Atomen.
Das Symbol innerhalb eines Relationsknotens gibt die Relation des Atoms an.
Für die Repräsentation der Argumente werden sog.~Argumentkanten zwischen Relationsknoten und Konzeptknoten verwendet.
Die Position der Argumente bei mehrstelligen Relationen werden durch Nummerierung der Argumentkanten oder bei zweistelligen Relationen durch gerichtete Argumentkanten abgebildet.
Wenn in einem Graphen mehrere Relationsknoten bzw.\ Atome auftauchen, werden diese als \textit{UND}-verknüpft interpretiert;
für die Abbildung von \textit{ODER} wird die Negation verwendet.
\begin{align*}
	\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/relationNode2.pdf}}}
	\Leftrightarrow
	\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/relationNode1.pdf}}}
	\quad\Leftrightarrow\quad
	\exists\ a, b:\ {\color{rot}P(a, b) \land R(b, a)} \numberthis
\end{align*}

\paragraph{Negationskontexte (\textit{negation contexts} oder \textit{cuts})}
Für die Negation von Aussagen werden in Konzeptgraphen sog.~Kontexte verwendet.
Sie lassen sich neben der Negation auch zur Modellierung anderer Zusammenhänge nutzen, diese werden hier allerdings ausgelassen, um den Vergleich mit der Prädikatenlogik zu ermöglichen.
\begin{align*}
	\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/negationNode1.pdf}}}
	\quad\Leftrightarrow\quad
	&\exists\ a\ {\color{rot}\lnot\exists}\ b: P(a, b) \numberthis \\
	\quad\Leftrightarrow\quad
	&\exists\ a\ {\color{rot}\forall}\ b: {\color{rot}\lnot} P(a, b)
\end{align*}
Die Darstellung von Kontexten mit Knoten und Kanten wird schnell unübersichtlich, daher werden stattdessen Boxen verwendet, die die Kindknoten umschließen.
\begin{align*}
	\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/negationNode1.pdf}}}
	\quad\Leftrightarrow\quad
	\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/negationNode2.pdf}}}
\end{align*}
Kontexte können nicht nur Konzeptknoten und Relationsknoten enthalten, sondern auch andere Kontexte.
Hierbei ist zu beachten, dass alle Knoten und Kontexte höchstens einen Elternkontext haben können;
die Linien zweier Kontextboxen dürfen sich also nicht schneiden.
\begin{align*}
	\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/negationNode3.pdf}}}
	\quad\Leftrightarrow\quad
	&{\color{rot}\lnot\exists}\ a\ {\color{rot}\lnot\exists}\ b: P(a, b) \numberthis \\
	\quad\Leftrightarrow\quad
	&{\color{rot}\forall}\ a\ \exists\ b: P(a, b) \\[1ex]
	\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/negationNode4.pdf}}}
	\quad\Leftrightarrow\quad
	&\exists\ a, b: {\color{rot}\lnot}({\color{rot}\lnot} P(a, b) \land {\color{rot}\lnot} R(b, a)) \numberthis \\
	\quad\Leftrightarrow\quad
	&\exists\ a, b: P(a, b)\ {\color{rot}\lor}\ R(b, a)
\end{align*}

\paragraph{Koreferenzkanten (\textit{coreference links})}
Entspricht der Äquivalenzrelation.
\begin{align}
	\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/coreferenceEdge.pdf}}}
	\quad\Leftrightarrow\quad
	&\exists\ a, b: P(a, b) \land \lnot\exists\ x: {\color{rot}a = x \land x = b} \displaybreak[0]\\
	\quad\Leftrightarrow\quad
	&\exists\ a, b: P(a, b) \land {\color{rot}a \neq b} \nonumber
\end{align}
Prinzipiell ließe sich die Äquivalenz auch durch Relationsknoten ausdrücken.
Um syntaktisch zu kennzeichnen, dass es sich nicht um eine beliebige Relation, sondern um eine Äquivalenzrelation handelt, wird dies jedoch i.~d.~R. nicht getan.
Koreferenzkanten können also als eine Kurzschreibweise verstanden werden, die den Zweck hat, die für die Inferenz relevanten Symmetrie-, Transitivitäts- und Reflexivitätseigenschaften zu kennzeichnen.
\begin{align*}
	\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/coreferenceEdge.pdf}}}
	\quad\Leftrightarrow\quad
	\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/coreferenceEdgeAlternative.pdf}}}
\end{align*}

\subsection{Definition}%
\label{sec:theory:cg:definition}

Im vorigen Abschnitt wurde die Konzeptgraphsyntax und deren Semantik beschrieben.
Bislang ist allerdings unklar, welche Kombinationen dieser Syntaxelemente erlaubt sind.
So wie die Syntaxelemente prädikatenlogischer Ausdrücke nicht beliebig kombiniert werden können, unterliegen auch Konzeptgraphen gewissen Einschränkungen.
In diesem Abschnitt werden die Eigenschaften beschrieben, die ein Konzeptgraph erfüllen muss.
Ein Konzeptgraph $G = (V, E)$ ist ein gerichteter Graph mit den folgenden Eigenschaften:
\begin{align*}
	V =\ &\{ v: concept(v) \} \mathbin{\dot\cup} \{ v: relation(v) \} \mathbin{\dot\cup} \{ v: context(v) \} \\
	E =\ &\{ e: \mathit{coref}(e) \} \mathbin{\dot\cup} \{ (r, v): \exists\, i: \mathit{arg}(r, v, i) \} \mathbin{\dot\cup} \{ e: nest(e) \} \\ % chktex 21 chktex 35
	concept(v) :\Leftrightarrow\ &\text{$v$ ist ein Konzeptknoten} \\
	relation(v) :\Leftrightarrow\ &\text{$v$ ist ein Relationsknoten} \\
	context(v) :\Leftrightarrow\ &\text{$v$ ist ein Kontext, es gilt $context(\top)$} \\
	neg(v) :\Leftrightarrow\ &context(v) \land \text{$v$ ist ein Negationskontext} \displaybreak[0]\\
	\mathit{coref}(v_1, v_2) :\Leftrightarrow\ &concept(v_1) \land concept(v_2) \land \text{$v_1$ und $v_2$ sind koreferent} \\
	\mathit{arg}(r, v, i) :\Leftrightarrow\ &relation(r) \land concept(v) \land \text{$v$ ist $i$-tes Argument von $r$}  \\ % chktex 35
	nest(c, v) :\Leftrightarrow\ &context(c) \land \text{$c$ ist Elternkontext von $v$} \\
	& \forall v \in V: (\exists\, c \in V \setminus \{ \top \}: nest(c, v)) \lor nest(\top, v) \label{eq:theory:kgcrit0}\numberthis \\ % chktex 21
	& (V, \{ (c, v): nest(c, v) \land v \neq \top \}) \text{ ist zyklenfrei} \label{eq:theory:kgcrit1}\numberthis \\ % chktex 21
	& \lnot \exists\, c_1, c_2, a: c_1 \neq c_2 \land nest(c_1, a) \land nest(c_2, a) \label{eq:theory:kgcrit2}\numberthis % chktex 21
\end{align*}
Zentral in dieser Definition sind die Kriterien~\eqref{eq:theory:kgcrit0},~\eqref{eq:theory:kgcrit1} und~\eqref{eq:theory:kgcrit2}.
Sie fordern, dass der Teilgraph $T = (V, \{ (c, v): nest(c, v) \land v \neq \top \})$ ein gerichteter Baum mit Wurzel $\top$ ist.
Dabei ist $\top$ der sog.\ globale Kontext, der, in Anlehnung an Peirce, auch \textit{Sheet of Assertion} genannt wird.
$\top$ enthält alle Knoten, die keinen explizit dargestellten Elternkontext haben.
Der \textit{Elternkontext} eines Knotens $v$ ist dabei der Kontext $c$, für den $nest(c, v)$ gilt.
Die sog.\ \textit{umgebenden Kontexte} eines Knotens $v$ sind alle Kontexte $c$, für die ein Pfad $c \rightarrow v$ in $T$ existiert.

Zusätzlich zu den obigen Kriterien, muss jeder Konzeptgraph $G$ die Eigenschaft dominierender Knoten $dom(G)$ erfüllen.
Um diese Eigenschaft zu beschreiben, wird zuerst die Quasiordnung $\leq$ eingeführt:
\begin{align*}
	a \leq b :\Leftrightarrow\ & \quad nest(b, a) \numberthis \\
	& \lor (\exists\ c \in V: nest(c, a) \land nest(c, b)) \\
	& \lor (\exists\ x \in V: a \leq x \land x \leq b)
\end{align*}
Es gilt $a \leq b$, wenn $b$ ein Kontext ist, der $a$ enthält, oder wenn $a$ und $b$ denselben Elternkontext haben, oder wenn aufgrund der Ordnungs-Transitivität $a \leq x \leq b$ gilt.
Das größte Element gemäß $\leq$ ist also immer $\top$.
Zusammenfassend beschreibt $a \leq b$, dass der Elternkontext von $b$ umgebender Kontext von $a$ ist.
In \figreft{fig:theory:kgorder} wird dieser Zusammenhang veranschaulicht.\\[-3em]
\begin{figure}[h]
	\centering
	\[\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/contextTreeExample1.pdf}}}
	\qquad
	\vcenter{\hbox{
		\begin{tikzpicture}[
			grow=down,
			sloped,
			level distance=2em,
			sibling distance=3em,
			edge from parent/.style={draw=black!70,latex-}]
			\node {\color{rot}$\top$}
				child {
					node (a) {$a$}
				}
				child {
					node (c1) {\color{rot}$c_1$}
						child {
							node (c2) {\color{rot}$c_2$}
								child {
									node {$P$}
								}
						}
						child {
							node (c3) {\color{rot}$c_3$}
								child {
									node {$R$}
								}
						}
				}
				child {
					node (b) {$b$}
				};
				\draw[latex-latex] (a) -- (c1);
				\draw[latex-latex] (c1) -- (b);
				\draw[latex-latex] (c2) -- (c3);
		\end{tikzpicture}
	}}\]
	\caption[Zusammenhang zwischen Kontexten und der $\leq$-Ordnung]{Zusammenhang zwischen Kontexten und der $\leq$-Ordnung. Die Existenz eines Pfades von $x$ nach $y$ im obigen baumartigen Graphen, entspricht $x \leq y$.}\label{fig:theory:kgorder}
\end{figure}\\
Auf Basis von $\leq$ lässt sich nun $dom$ definieren:
\begin{align*}
	dom(G) :\Leftrightarrow\ & \quad \forall\ r, v \in V, i \in \mathbb{N}: \mathit{arg}(r, v, i) \rightarrow r \leq v \numberthis \\ % chktex 35
	& \land \forall\ v_1, v_2 \in V: \mathit{coref}(v_1, v_2) \rightarrow (v_1 \leq v_2 \lor v_2 \leq v_1)
\end{align*}
Eine Intuition für diese Anforderung an Konzeptgraphen findet sich in der Semantik, die die durch $dom$ verbotenen Kanten hätten:
Sie würden die Existenz eines Atoms auszudrücken, welches durch nicht existente Variablen parametrisiert ist;
dies ist nicht sinnvoll.
Eine detailliertere Untersuchung des Zwecks dominierender Knoten und eine Beschreibung der entstehenden Probleme, wenn auf die Notwendigkeit dominierender Knoten verzichtet wird, findet sich in~\cite[Abschnitt~14.3]{Dau2003}.
\begin{figure}[h]
	\begin{align*}
		\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/dominatingNodeViolation1.pdf}}}
		\quad
		&\text{\color{rot}$\lightning\ \lnot dom(G)$, da $\mathit{arg}(P, b) \land \lnot (P \leq b)$.} \\ % chktex 35
		\vcenter{\hbox{\includegraphics[scale=\cgCutScale]{gfx/theory/dominatingNodeViolation2.pdf}}}
		\quad
		&\text{\color{rot}$\lightning\ \lnot dom(G)$, da $\mathit{coref}(a, b) \land \lnot (a \leq b \lor b \leq a)$.}
	\end{align*}
	\caption{Beispiele für fehlerhafte Konzeptgraphen ohne dominierende Knoten}\label{fig:theory:invalidkg}
\end{figure}

\section{Stanford CoreNLP}%
\label{sec:theory:nlp}

Um natürlichsprachliche Daten in einen Konzeptgraphen zu transformieren, ist im ersten Schritt eine Sprachanalyse notwendig.
Hierfür wurde die Stanford CoreNLP~\cite{CoreNLP} und die Apache OpenNLP~\cite{OpenNLP} Bibliothek in Erwägung gezogen, da beide häufig genutzt, aktiv weiterentwickelt, frei verfügbar und JVM-basiert sind.
Die JVM-Integration ist wichtig, um mit anderen verwendeten Bibliotheken kompatibel zu sein;
mehr hierzu in \treft{sec:text2kg:implementation}.
Für die Implementation wurde schließlich CoreNLP gewählt, da es mit den mitgelieferten Modellen häufig bessere Ergebnisse als OpenNLP liefert.
Da beide Bibliotheken bzgl.\ ihrer Funktionalität allerdings recht ähnlich sind, kann die NLP Komponente als substituierbar angesehen werden.
Ein Wechsel von CoreNLP auf OpenNLP wäre mit relativ geringem Aufwand möglich.

Im Folgenden wird nun die grundlegende Architektur von CoreNLP beschrieben.
CoreNLP verwendet das in~\ref{sec:related:nlp} vorgestellte Pipeline-Modell.
Die verschiedenen Verarbeitungsstufen der Pipeline werden Annotatoren genannt.
Die genaue Funktionsweise der Annotatoren ist für diese Arbeit weniger relevant, wichtiger ist ein Überblick über die Art der Ergebnisse, die die Annotatoren liefern.

\paragraph{Tokenization und Lemmatization}
Diese Annotatoren liefern, wie erwartet, eine Liste von Token bzw.\ eine Liste der Lemmata der Token.
Es werden neben Englisch zahlreiche weitere Sprachen und ein Großteil des Unicode Zeichensatzes unterstützt.
Der Tokenizer verwendet zum Finden der Token intern einen deterministischen endlichen Automaten.

\paragraph{POS-Tagging}
Dieser Annotator ordnet jedem Token eine Wortart und Flexion (POS-Tag) zu.
Die Menge der möglichen POS-Tags wurde aus dem \textit{Penn Treebank Tag Set}~\cite{Santorini1990} übernommen.
Für das Finden der Tags benutzt CoreNLP sog.\ \textit{Cyclic Dependency Networks}~\cite{Toutanova2003}, eine Erweiterung bayesscher Netze, in denen zyklische Abhängigkeiten erlaubt sind.

\paragraph{Named Entity Recognition (NER)}
Findet sog.\ Entitäten.
CoreNLP benutzt hierfür eine Menge von Entitätsklassen, die sich in drei Kategorien von Klassen unterteilen lässt:
\begin{enumerate}
	\item \textbf{Benannte Entitäten:}
		Person, Ort, Organisation und Sonstige.
		Diese Entitätsklassen werden mittels \textit{Conditional Random Fields}~\cite{Finkel2005}, einer Variante von \textit{Markov Random Fields}~\tref{sec:theory:psl:mrf}, erkannt.
	\item \textbf{Numerische Entitäten:}
		Geldbetrag, Zahl, Ordinalzahl und Prozentzahl.
		Hierfür wird ein regelbasiertes System verwendet.
		Die so erkannten Token werden zudem normalisiert, um eine leichtere Weiterverarbeitung zu ermöglichen.
	\item \textbf{Temporale Entitäten:}
		Datum, Uhrzeit, Dauer und Menge von Zeitpunkten.
		Diese Klassen werden ebenfalls mit einem regelbasierten System erkannt.
		Mittels SUTime~\cite{Chang2012} werden die erkannten Token anschließend normalisiert und relative Zeitangaben in absolute Zeitpunkte aufgelöst, sofern ein Referenzzeitpunkt gegeben ist.
		Für die Normalisierung wird das TimeML TIMEX3-Format~\cite{TIMEX3} benutzt, mit dem sich auch komplexe Zeitangaben, wie \textit{``twice a week''} (\texttt{type=''set'' value=''P1W'' freq=''2X''}), formal ausdrücken lassen.
\end{enumerate}

\paragraph{Coreference Resolution}
Ermittelt Äquivalenzklassen von Token, die auf dasselbe Konzept bzw.\ dieselbe Entität verweisen.
CoreNLP stellt hierfür drei verschiedene Systeme bereit:
Ein schnelles, regelbasiertes, deterministisches System, ein etwas langsameres statistisches System und zuletzt ein langsames System, das auf neuronalen Netzen (NN) basiert.
Das regelbasierte System liefert im Schnitt die schlechtesten Ergebnisse, das NN-System die besten~\cite{CoreNLPCoref}.

\paragraph{Dependency Parsing}
Dieser Annotator ermittelt die grammatikalischen Beziehungen zwischen Worten.
Das Ergebnis ist ein sog.\ Abhängigkeitsgraph (\textit{Dependency Graph}), in dem die Knoten Token und die Kanten Beziehungen repräsentieren.
CoreNLP verwendet für die Kantentypen \textit{Universal Dependencies Version 2}~\cite{UDv2}, eine Menge von 37 Arten grammatikalischer Beziehungen, die für eine vielzahl natürlicher Sprachen nutzbar ist.
Die Struktur der zurückgegebenen Abhängigkeitsgraphen, basiert auf einem ``\textit{{\color{blau}head}-{\color{rot}modifier}}''-Pattern;
d.~h.\ dass, ausgehend von einem \textit{\color{blau}head}-Token, Kanten zu \textit{\color{rot}modifier}-Token gehen, die die Bedeutug des \textit{\color{blau}heads} verändern.
\[
	\text{\color{rot}Peter's} \xleftarrow{\text{possessive nominal modifier}}
	\text{\color{blau}ball}
	\xrightarrow{\text{adjectival modifier}} \text{\color{rot}red}
\]
Der CoreNLP Dependency Parser nutzt ein sog.\ \textit{Transition-based Parsing}~\cite{Nivre2004}, bei dem alle Token der Reihe nach aus einem Buffer auf einen Stack von aktuell betrachteten Token gelegt werden.
Ein Klassifikator (im Falle von CoreNLP ist dies ein neuronales Netz) wählt dabei in jedem Schritt einen von drei Zustandsübergängen:
\begin{enumerate}
	\item \textbf{LEFT-ARC:}
		Fügt eine Abhängigkeitskante $(i, j)$ vom ersten Token $i$ des Stacks zum zweiten Token $j$ des Stacks ein und entfernt dann $j$ vom Stack.
	\item \textbf{RIGHT-ARC:}
		Fügt eine Abhängigkeitskante $(j, i)$ vom zweiten Token $j$ des Stacks zum ersten Token $i$ des Stacks ein und entfernt dann $i$ vom Stack.
	\item \textbf{SHIFT:}
		Verschiebt das erste Token des Buffers auf den Stack.
\end{enumerate}
Diese drei Zustandsübergänge werden so lange angewandt, bis der Buffer leer ist.
Durch die richtige Kombination von Übergängen lässt sich jeder beliebige Abhängigkeitsgraph beschreiben.

\section{Modellierung von HL-MRFs mit PSL}%
\label{sec:theory:psl}

In~\ref{sec:theory:cg} wurde beschrieben, wie komplexes Wissen durch Konzeptgraphen repräsentiert werden kann;
in~\ref{sec:theory:nlp} wurde beschrieben, wie der Inhalt natürlichsprachlicher Texte extrahiert und durch eine Menge von Abhängigkeiten repräsentiert werden kann.
Dieser Abschnitt beschreibt nun, wie aus einer Menge gegebenener Abhängigkeiten und Fakten neue Abhängigkeiten und Fakten inferiert werden können.
Konkrekt werden hierfür \textit{Hinge-Loss Markov Random Fields} (HL-MRFs) und die \textit{Probabilistic Soft Logic} (PSL) vorgestellt.

\subsection{Markov Random Fields}%
\label{sec:theory:psl:mrf}

MRFs~\cite[Kapitel 19]{Murphy2012} sind, so wie auch bayessche Netze, eine Klasse von \textit{Probabilistischen Graphischen Modellen} (PGM);
d.~h.\ sie sind Graphen, deren Knoten als Zufallsvariablen und deren Kanten als stochastische Abhängigkeiten interpretiert werden.
Im Gegensatz zu bayesschen Netzen sind die Kanten in MRFs allerdings ungerichtet, es sind also zyklische Abhängigkeiten erlaubt.
Formal ist ein MRF ein ungerichteter Graph $G$, in dem die Knoten Zufallsvariablen sind:
\begin{align*}
	X :=&\ \text{Zufallsvektor } (X_1, \dots, X_n) \\
	G :=&\ (\{ X_1, \dots, X_n \}, E)
\end{align*}
$G$ ist ein MRF, gdw.\ die Zufallsvariablen $X_1, \dots, X_n$ die folgenden sog.~Markov-Eigenschaften erfüllen:
\begin{enumerate}
	\item \textbf{Globale Markov-Eigenschaft:}
		\[sep_{X_A, X_B}(X_S) \Rightarrow X_A \perp X_B \mid X_S\]
		Alle Paare $(X_A, X_B)$ von Teilmengen von $X$ sind bedingt unabhängig, sofern die Werte einer separierenden Teilmenge $X_S $ gegeben sind.
		$X_S$ separiert $(X_A, X_B)$ ($sep_{X_A, X_B}(X_S)$), wenn alle Pfade von $a \in X_A$ nach $b \in X_B$ einen Knoten $s \in X_S$ enthalten.
	\item \textbf{Lokale Markov-Eigenschaft:}
		\[X_i \perp (X \setminus \Gamma(X_i) \setminus \{X_i\}) \mid \Gamma(X_i)\]
		Eine direkte Konsequenz der globalen Markov-Eigenschaft ist die lokale Markov-Eigenschaft.
		Jede Variable $X_i$ ist bedingt unabhängig von ihren nicht benachbarten Variablen, sofern ihre Nachbarschaft $\Gamma(X_i)$ gegeben ist.
	\item \textbf{Paarweise Markov-Eigenschaft:}
		\[\{ X_i, X_j \} \notin E \Rightarrow X_i \perp X_j \mid X \setminus \{ X_i, X_j \}\] % chktex 21
		Aus der lokalen Markov-Eigenschaft folgt, dass jedes nicht adjazente Variablenpaar $(X_i, X_j)$ bedingt unabhängig voneinander ist, sofern alle anderen Variablen gegeben sind.
\end{enumerate}

\paragraph{Faktorisierung}
Eine häufig benutzte Methode, um die Verteilung $P(X = x)$ eines MRFs $G$ zu beschreiben ist die sog.\ Cliquen-Faktorisierung:
\begin{align*}
	\mathcal{C} :=&\ \{ c_1, \dots, c_m \} = \text{Menge der maximalen Cliquen in $G$} \\ % chktex 21
	X_c :=&\ \text{Vektor der Zufallsvariablen in der Clique $c \in \mathcal{C}$} \\
	\Phi_c(x_c) :=&\ \text{Cliquenpotential $\in \mathbb{R}^{+}_0$ der Werte $x_c$ von $X_c$} \\
	P(X = x) =&\ \frac{1}{Z} \prod_{i = 1}^{m} \Phi_{c_i}(x_{c_i}) \text{, mit Normalisier.konst. } Z := \sum_{x \in \mathcal{X}} \prod_{i = 1}^{m} \Phi_{c_i}(x_{c_i}) \numberthis
\end{align*}
Es lässt sich zeigen, dass für jedes MRF mit einer Verteilung $P'$ mit positiver Dichte eine Potentialfunktion $\Phi$ existiert, sodass $P' = P$ gemäß der obigen Definition~\cite{Bilmes2006}.

\paragraph{Beispiel}
Die Beschreibungen von MRFs und deren Faktorisierung sind bislang noch recht abstrakt.
Ein exemplarisches praktisches Einsatzgebiet von MRFs ist das Lösen von SAT-Problemen~\cite{Bach2015a}.
Gegeben sei die SAT-Instanz ${\color{rot}(\lnot A \lor B \lor C)} \land {\color{blau}(\lnot C \lor \lnot D)}$.
Ein Cliquen-faktorisiertes MRF kann benutzt werden, um dieses Problem zu modellieren und eine erfüllende Belegung zu finden.\\
\begin{tabular}{m{0.25\textwidth} m{0.75\textwidth}}
	\includegraphics[width=0.23\textwidth]{gfx/theory/mrfExample1.pdf}
	&
	{\begin{align*}
		X :=&\ (A, B, C, D) \text{, mit } Bild(X) = {\{0, 1\}}^4 \\
		\mathcal{C} :=&\ \{{\color{rot}\underbrace{\{ A, B, C \}}_{c_1}}, {\color{blau}\underbrace{\{ C, D \}}_{c_2}}\} \\ % chktex 21
		{\color{rot}\Phi_{c_1}}(a, b, c) :=&\ \min\{ (1 - a) + b + c, 1 \} \\ % chktex 21
		{\color{blau}\Phi_{c_2}}(c, d) :=&\ \min\{ (1 - c) + (1 - d), 1 \} \\ % chktex 21
		P(X = (a, b, c, d)) :=&\ \frac{1}{Z} {\color{rot}\Phi_{c_1}}(a, b, c)\ {\color{blau}\Phi_{c_2}}(c, d)
	\end{align*}}
\end{tabular}\\
Eine Clique repräsentiert in diesem MRF eine Disjunktionsklausel und das Cliquenpotential gibt an, ob eine gegebene Belegung die Klausel erfüllt.
Gemäß dieser Definition, lässt sich die Erfüllbarkeit durch $\max_{x} P(X = x) > 0$ ausdrücken, d.~h.\ die Formel ist erfüllbar, gdw.\ es eine Variablenbelegung mit Eintrittswahrscheinlichkeit~$> 0$ gibt.
Die Normalisierungskonstante $Z$ hat auf das Ergebnis keinen Einfluss und kann daher ignoriert werden.

In diesem sehr einfachen Beispiel konnte jede Klausel eindeutig auf eine Clique abgebildet werden.
Dies ist nicht immer der Fall; fügt man dem obigen Beispiel die Klausel $(\lnot B)$ hinzu, muss ${\color{rot}\Phi_{c_1}}(a, b, c) = (1 - b) \min\{ (1 - a) + b + c, 1 \}$ gesetzt werden. % chktex 21
Falls es also eine Klausel gibt, die alle Variablen der SAT-Instanz enthält, und es somit nur eine Clique gibt, sind alle Zufallsvariablen voneinander abhängig.

\paragraph{Das MRF-Inferenzproblem}
Da sich SAT, wie soeben exemplarisch gezeigt, auf MRFs reduzieren lässt, ist das Inferenzproblem, d.~h.\ das Finden einer maximal wahrscheinlichen Belegung der Zufallsvariablen, ein NP-schweres Problem.
Allgemeine, exakte und effiziente Lösungsalgorithmen existieren daher nicht.
Durch Einschränken der Struktur von $G$ und $\Phi$, oder durch das Erlauben von Approximationen, lassen sich MRF-Inferenzen jedoch deutlich effizienter durchführen.
Wenn z.~B. $G$ ein Baum ist, es also nur 2-Cliquen gibt, kann mit dem \textit{Belief Propagation} Algorithmus eine exakte Lösung in polynomieller Zeit gefunden werden (vgl.\ 2-SAT~\cite{2SAT}).

\subsection{Hinge-Loss MRFs}%
\label{sec:theory:psl:hlmrf}

Eine Unterart von MRFs, sind die sog.\ Hinge-Loss MRFs~\cite{Bach2013}.
Sie sind so strukturiert, dass sich das Inferenzproblem effizient und exakt durch konvexe Optimierungsverfahren lösen lässt.
Es gibt drei wesentliche Unterschiede zu allgemeinen MRFs:
\begin{enumerate}
	\item
		Die Bedeutung von Zufallsvariablen und Kanten zwischen Zufallsvariablen ist klar definiert, da $\Phi$ nicht mehr frei wählbar ist.
		Ähnlich zum Beispiel aus~\ref{sec:theory:psl:mrf}, repräsentieren Zufallsvariablen in HL-MRFs aussagenlogische Variablen.
		Zufallsvariablen sind adjazent, gdw.\ sie in einer gemeinsamen Disjunktionsklausel vorkommen.
	\item
		Für den Zufallsvektor $X$ muss $Bild(X) = {[0, 1]}^n$ gelten.
		Diese Einschränkung besteht, da jede HL-MRF-Zufallsvariable als die Wahrscheinlichkeit interpretiert wird, dass eine aussagenlogische Variable wahr ist.
	\item
		Die Verteilung $P$ wird etwas anders faktorisiert:
		\begin{align*}
			P(X = x) :=&\ \frac{1}{Z} \prod_{i = 1}^{m} e^{w_i\,\Phi_i(x)} \propto \exp\left(\sum_{i=1}^{m} w_i\,\Phi_i(x)\right) = \exp\left(w^\top {\Phi(x)}\right) \numberthis \\
			w :=&\ (w_1, \dots, w_m) \in {\mathopen[0, \infty\mathclose)}^m,\ \Phi := (\Phi_1, \dots, \Phi_m) % chktex 9
		\end{align*}
		Auf die Potentiale $\Phi_i$ wird nun die Exponentialfunktion angewandt, zudem erhalten alle Potentiale ein Gewicht $w_i$.
		Das Inferenzproblem $\arg\max_{x} P(X = x)$ ist somit äquivalent zu $\arg\max_{x} w^\top {\Phi(x)}$.
\end{enumerate}

\paragraph{KNF-Formel Interpretation}
Aufgrund der Einschränkung von HL-MRFs auf aussagenlogische Ausdrücke, wird im Folgenden die Graphterminologie fallen gelassen und stattdessen die entsprechende aussagen\-logische Terminologie verwendet.
Ein HL-MRF wird nun als Repräsentation einer KNF-Formel $C_1 \land \dots \land C_m$ interpretiert.
Jede Disjunktionsklausel $C_j \in C$ wird durch eine Menge von Variablenindizes positiver Atome $I^{+}_j \subseteq \{1,\dots,n\}$ und eine Menge von Variablenindizes negativer Atome $I^{-}_j \subseteq \{1,\dots,n\}$ beschrieben.
\[
	C_j \cong \left(\bigvee_{i \in I^{+}_j} X_i\right) \lor \left(\bigvee_{i \in I^{-}_j} \lnot X_i\right)
\]
Statt jeder Clique ein Potential zuzuordnen, wird in HL-MRFs jeder Klausel ein Potential zugeordnet;
dies wird getan, um die Potentiale einfacher zu halten.
Zwei Klauseln mit denselben Variablen werden also durch zwei Potentiale beschrieben, statt, wie in Cliquen-faktorisierten MRFs, durch eines.

\paragraph{Łukasiewicz Logik}
Da die Variablen der KNF-Formel, gemäß obiger Definition, Werte~$\in [0, 1]$ annehmen können, ist nun noch unklar, wie die Operatoren $\land$, $\lor$ und $\lnot$ funktionieren sollen.
HL-MRFs benutzen hierfür die sog.\ Łukasiewicz Logik aus der Klasse der T-Norm Fuzzy Logiken;
sie ist eine Erweiterung der booleschen Logik, d.~h.\ die Łukasiewicz Operatoren verhalten sich für die Extrema $0$ und $1$ so, wie die booleschen Operatoren, sind aber ebenfalls für alle dazwischen liegenden Eingabewerte definiert.
\begin{align}
	x_1 \land x_2 :=&\ \max\{ x_1 + x_2 - 1, 0 \} \\ % chktex 21
	x_1 \lor x_2 :=&\ \min\{ x_1 + x_2, 1 \} \\ % chktex 21
	\lnot x :=&\ 1 - x
\end{align}
\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			title={$\land$},
			xlabel=$x_1$,
			ylabel=$x_2$,
			width=0.35\textwidth,
			colormap = {bluered}{color(0cm) = (blau); color(1cm) = (rot)}
		]
			\addplot3[
				mesh,
				samples=12,
				domain=0:1,
				domain y=0:1
			]{max(x + y - 1, 0)};
		\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{axis}[
			title={$\lor$},
			xlabel=$x_1$,
			ylabel=$x_2$,
			width=0.35\textwidth,
			colormap = {bluered}{color(0cm) = (blau); color(1cm) = (rot)}
		]
			\addplot3[
				mesh,
				samples=12,
				domain=0:1,
				domain y=0:1
			]{min(x + y, 1)};
		\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{axis}[
			title={$\lnot$},
			xlabel=$x$,
			width=0.35\textwidth,
			colormap = {bluered}{color(0cm) = (blau); color(1cm) = (rot)}
		]
			\addplot[
				surf,
				domain=0:1
			]{1 - x};
		\end{axis}
	\end{tikzpicture}
	\caption{Visualisierung der Łukasiewicz Operatoren $\land$, $\lor$ und $\lnot$}\label{fig:theory:luklogic}
\end{figure} \\
Mittels der Łukasiewicz Logik können Disjuktionsklauseln $C_j \in C$ für eine gegebene Variablenbelegung $x$ nun Wahrheitswerte~$\in [0, 1]$ zugeordnet werden.
Dieser Wahrheitswert wird als Klauselpotential $\Phi_j(x)$ verwendet.
Das HL-MRF-Inferenzproblem für KNF-Formeln in Łukasiewicz Logik ist demnach
\begin{alignat*}{3}
	\operatorname*{arg\,max}_{x \in {[0, 1]}^n}& \sum_{C_j \in C} w_j &&\Phi_j(x) \displaybreak[0]\\ % chktex 35
	= \operatorname*{arg\,max}_{x \in {[0, 1]}^n}& \sum_{C_j \in C} w_j &&\left(\left(\bigvee_{i \in I^{+}_j} x_i\right) \lor \left(\bigvee_{i \in I^{-}_j} \lnot x_i\right)\right) \displaybreak[0]\\ % chktex 35
	= \operatorname*{arg\,max}_{x \in {[0, 1]}^n}& \sum_{C_j \in C} w_j \min &&\left\{ \left(\sum_{i \in I^{+}_j} x_i\right) + \left(\sum_{i \in I^{-}_j} (1 - x_i)\right), 1 \right\} \numberthis % chktex 21, chktex 35
\end{alignat*}

Statt die Summe der Wahrheitswerte $\Phi_j(x)$ zu maximieren, kann alternativ auch die Summe der Distanzen zur Erfüllung $\ell_j(x)$, genannt \textit{Distance to Satisfaction}, minimiert werden;
es gilt $\ell_j(x) = 1 - \Phi_j(x)$.
Gemäß dieser Interpretation ist ein HL-MRF somit eine Menge von gewichteten Constraints $\ell_j(x) \leq 0$, für die eine Lösung mit möglichst wenigen Verletzungen dieser Constraints gesucht wird.
Das Inferenzproblem lässt sich also als das Finden des folgenden Minimums beschreiben:
\begin{align*}
	\operatorname*{arg\,min}_{x \in {[0, 1]}^n}& \sum_{C_j \in C} w_j \max \{ \ell_j(x), 0 \} \displaybreak[0]\\ % chktex 35
	= \operatorname*{arg\,min}_{x \in {[0, 1]}^n}& \sum_{C_j \in C} w_j \max \left\{ 1 - \left(\sum_{i \in I^{+}_j} x_i\right) - \left(\sum_{i \in I^{-}_j} (1 - x_i)\right), 0 \right\} \numberthis % chktex 21, chktex 35
\end{align*}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			xlabel=$x_1$,
			ylabel=$x_2$,
			y dir=reverse,
			width=0.45\textwidth,
			colormap = {bluered}{color(0cm) = (blau); color(1cm) = (rot)}
		]
			\addplot3[
				mesh,
				samples=20,
				domain=0:1,
				domain y=0:1
			]{max(1 - x - y, 0)};
		\end{axis}
	\end{tikzpicture}
	\caption{Visualisierung der Loss-Funktion $\ell_j(x_1, x_2)$ für $C_j \cong X_1 \lor X_2$}\label{fig:theory:hingeloss}
\end{figure}
Wie \figreft{fig:theory:hingeloss} für den zwei-elementigen Klauselfall veranschaulicht, handelt es sich bei $\ell$ um eine Hinge-Loss Funktion.
Hierher rührt die Bezeichnung Hinge-Loss MRF.\@
Da Hinge-Loss Funktionen konvex sind und die Summe konvexer Funktionen ebenfalls konvex ist, handelt es sich bei der HL-MRF-Inferenz um ein konvexes Optimierungsproblem.
Es existieren also effiziente und exakte Lösungsalgorithmen.
Einer dieser Algorithmen ist das \textit{Alternating Direction Method of Multipliers} Verfahren~(ADMM), es wird in~\ref{sec:theory:psl:inference} näher vorgestellt.

\paragraph{MAX-SAT Äquivalenz}
In~\ref{sec:theory:psl:mrf} wurden MRFs anhand des Beispiels der SAT-Instanz ${\color{rot}(\lnot A \lor B \lor C)} \land {\color{blau}(\lnot C \lor \lnot D)}$ veranschaulicht.
Diese KNF-Formel hat folgendes Inferenzproblem, wenn sie als HL-MRF repräsentiert wird:
\[
	\operatorname*{arg\,min}_{(a, b, c, d) \in {[0, 1]}^4} {\color{rot}w_1 \max\{a - b - c, 0\}} + {\color{blau}w_2 \max\{c + d - 1, 0\}} % chktex 21, chktex 35
\]
Da in der Verteilung $P$ von HL-MRFs die Exponentialfunktion auf die Potentiale angewandt wird, bewirkt eine unerfüllte Klausel mit $\Phi_j(x) = 0$ nicht, dass $P(X = x) = 0$.
Stattdessen wächst $P(X = x)$ mit der Summe der Wahrheitswerte der Klauseln.
Je mehr Klauseln erfüllt sind, desto größer ist $P(X = x)$.
Die HL-MRF-Inferenz beschreibt also nicht SAT, sondern eine Fuzzy-Logik-Entsprechung von MAX-SAT.\@
Ein wichtiger Unterschied zum booleschen MAX-SAT ist, dass Klauseln gewichtet sind;
das Erfüllen einer Klausel mit hohem Gewicht kann das Nicht-Erfüllen mehrerer anderer Klauseln mit niedrigem Gewicht ausgleichen.

\subsection{Probabilistic Soft Logic}%
\label{sec:theory:psl:psl}

Wie soeben beschrieben, sind HL-MRFs ein flexibles Werkzeug, um Probleme zu lösen, die sich durch MAX-SAT ausdrücken lassen.
Der Schritt von einem konkreten domänenspezifischen Problem in eine Menge von Klauseln $C$ und einen Gewichtsvektor $w$ ist bislang allerdings noch unklar.
An dieser Stelle setzt die \textit{Probabilistic Soft Logic}~(PSL)~\cite{Broecheler2010}\cite{Bach2015} an.
PSL ist eine formale Sprache, um Klassen von HL-MRFs mit einer intuitiven Syntax zu beschreiben.

\subsubsection{PSL Syntax}
Die Syntax von PSL ist an die Prädikatenlogik angelehnt und besteht aus sieben Arten von Elementen:
\begin{enumerate}
	\item \textbf{Konstanten:}
		Repräsentieren konstante Werte, wie z.~B. Strings oder Zahlen.
		Sie werden für domänenspezifische Daten wie z.~B. Namen benutzt.
	\item \textbf{Variablen:}
		Werden während einer Inferenz mit Konstanten belegt.
		PSL Variablen sind nicht zu verwechseln mit den Zufallsvariablen in MRFs.
	\item \textbf{Terme:}
		Ein Term ist entweder eine Konstante oder eine Variable.
	\item \textbf{Prädikate:}
		Entsprechen in etwa den prädikatenlogischen Prädikaten.
		Jedes PSL Prädikat hat einen eindeutigen Bezeichner und eine Signatur aus Konstanten\-typen.
		\[\mathit{Person}: \text{UUID},\quad \mathit{Name}: \text{UUID} \times \text{String}\]
	\item \textbf{Atome:}
		Ein Atom ist ein Prädikat der Arität $n$, kombiniert mit einem $n$-Tupel von Termen.
		Das Term-Tupel enthält die Argumente des Prädikates.
		Wenn alle Argumente Konstanten sind, spricht man von einem Grundatom (\textit{ground atom}). \\
		\[\mathit{Person}(x),\quad \mathit{Name}(x, \text{``Alice''})\] % chktex 32
	\item \textbf{Literale:}
		Ein Literal ist entweder ein Atom oder ein negiertes Atom. \\
		\[\mathit{Name}(x, \text{``Alice''}),\quad \lnot \mathit{Name}(x, \text{``Bob''})\] % chktex 32
	\item \textbf{Regeln:}
		Eine Regel ist eine gewichtete Disjunktionsklausel von Literalen.
		Die negativen Atome der Klausel bilden dabei den sog.\ Körper $B$ (\textit{body}), die positiven Atome den sog.\ Kopf $H$ (\textit{head}) der Regel.
		Die so zerlegte Disjunktionsklausel lässt sich als Implikationsregel interpretieren:
		\[
			\left(\bigvee_{b \in B} \lnot b\right) \lor \left(\bigvee_{h \in H} h\right) \Leftrightarrow \left(\bigwedge_{b \in B}  b\right) \rightarrow \left(\bigvee_{h \in H} h\right)
		\]
		Mit Implikationen lassen sich nun intuitiv Zusammenhänge zwischen Prädikaten modellieren.\\
		\[0.65: \mathit{Person}(x) \land \mathit{Name}(x, \text{``Alice''}) \rightarrow Female(x)\] % chktex 32, chktex 1
\end{enumerate}

Eine Menge von PSL-Regeln wird PSL-Programm genannt.
Als Input erwartet ein PSL-Programm $R$ eine sog.\ Basis $\mathcal{A}$.
Die Basis ist dabei eine Menge von Grundatomen, die während der Inferenz in Betracht gezogen werden sollen, und setzt sich aus zwei disjunkten Teilmengen $\mathbb{C} \mathbin{\dot\cup} \mathbb{O} = \mathcal{A}$ zusammen.
$\mathbb{C}$ ist die Menge der geschlossenen (\textit{closed}) Grundatome, d.~h.\ der Wahrheitswert~$\in [0, 1]$ dieser Atome ist bekannt.
$\mathbb{O}$ umfasst die offenen (\textit{open}) Grundatome, deren Wahrheitswerte noch unbekannt sind und daher durch das PSL-Programm inferiert werden sollen.

\paragraph{Beispiel}
Ein Anwendungsgebiet von PSL ist z.~B. die Modellierung sozialer Beziehungen.
Angenommen, es sollen Freundschaftsbeziehungen zwischen Personen inferiert werden.
Ein stark vereinfachtes PSL-Programm hierfür könnte so aussehen:
\begin{align*}
	0.4:&\ \lnot \mathit{Friends}(A, B) & \rulemark{rule:theory:prior} \\
	0.5:&\ \mathit{Friends}(A, B) \land \mathit{Friends}(B, C) \land A \neq C \rightarrow \mathit{Friends}(A, C) & \rulemark{rule:theory:trans} \\
	1:&\ \mathit{Interest}(A, X) \land \mathit{Interest}(B, X) \rightarrow \mathit{Friends}(A, B) & \rulemark{rule:theory:sim} \\
	\infty:&\ \mathit{Friends}(A, B) \rightarrow \mathit{Friends}(B, A) & \rulemark{rule:theory:sym} \\
	\infty:&\ \lnot \mathit{Friends}(A, A) & \rulemark{rule:theory:irref}
\end{align*}
In dieser Regelmenge sind fünf Annahmen über das Verhalten der $\mathit{Friends}$-Relation kodiert:
\begin{enumerate}[label=$(\ruleno{\arabic*})$]
	\item bildet ab, dass zwei Personen a~priori nicht miteinander befreundet sind.
		Eine solche PSL-Regel, die, in Ermanglung weiteren Wissens, das Nichtvorhanden\-sein einer Relation postuliert, wird \textit{Prior} genannt.
	\item bildet die Transitivität der $\mathit{Friends}$-Relation ab.
		Zwei Personen mit einem gemeinsamen Freund, sind evtl.\ befreundet.
		Das Infix-Prädikat $\neq$ ist üblicherweise vordefiniert.
	\item bildet ab, dass Personen mit einem gemeinsamen Interesse eine höhere Wahrscheinlichkeit haben befreundet zu sein.
	\item bildet die Symmetrie der $\mathit{Friends}$-Relation ab.
		Im Gegensatz zu den anderen Regeln, ist \ruleref{rule:theory:sym} ein sog.\ \textit{Constraint}, d.~h.\ sie hat ein Gewicht $w_4 = \infty$.
		Das Nichterfüllen von \ruleref{rule:theory:sym} hat somit zur Folge, dass die gewichtete Distance to Satisfaction ebenfalls den Wert $\infty$ annimmt und durch die Erfüllung anderer Regeln nicht ausgeglichen werden kann.
		Es ist also garantiert, dass jede MAX-SAT Lösung \ruleref{rule:theory:sym} einhält;
		d.~h. $\mathit{Friends}(A, B) = \mathit{Friends}(B, A)$.
	\item ist ein weiterer Constraint, der die Irreflexivität von $\mathit{Friends}$ erzwingt.
\end{enumerate}

Für das PSL-Programm $R = \{ \ruleno{1}, \dots, \rulecurrent \}$ sei nun folgender Input $\mathcal{A}$ gegeben:
\begin{align*}
	\mathbb{C} &= \left\{ % chktex 21
		\begin{aligned}
			\mathit{\color{rot}Interest}(\text{``Alice''}, \text{``Reading''}) &= 0.5,\ &\mathit{\color{rot}Interest}(\text{``Dave''}, \text{``Reading''}) &= 1, \\
			\mathit{\color{rot}Interest}(\text{``Alice''}, \text{``Skiing''}) &= 0.5,\ &\mathit{\color{rot}Interest}(\text{``Charlie''}, \text{``Skiing''}) &= 1, \\
			\mathit{\color{rot}Interest}(\text{``Bob''}, \text{``Tennis''}) &= 1,\ &\mathit{\color{rot}Interest}(\text{``Charlie''}, \text{``Tennis''}) &= 1, \\
			\mathit{\color{gruen}Friends}(\text{``Alice''}, \text{``Bob''}) &= 1
		\end{aligned}
	\right\} \\ % chktex 21
	\mathbb{O} &= \{\mathit{\color{blau}Friends}(x, y) =\ \textbf{?} : x, y \in \{\text{``Alice''}, \text{``Bob''}, \text{``Charlie''}, \text{``Dave''}\}\} \setminus \mathbb{C} % chktex 21
\end{align*}
Die Interessen der Personen und die Freundschaft zwischen Bob und Alice aus $\mathbb{C}$ werden als bekannt angenommen.
Die gesuchten Freundschaftsrelationen aus $\mathbb{O}$ sollen inferiert werden.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{gfx/theory/pslExample1.pdf}
	\caption[Graph der inferierten $\mathit{Friends}$-Relation und der gegebenen $\mathit{Interest}$-Relation]{Graph der inferierten {\color{blau}$\mathit{Friends}$-Relation} und der gegebenen {\color{rot}$\mathit{Interest}$-Relation}}\label{fig:theory:pslExample1}
\end{figure}
\figreft{fig:theory:pslExample1} zeigt ein mögliches MAX-SAT Ergebnis der Inferenz.
Wegen des Priors $r_1$ werden Personen generell als nicht befreundet eingeordnet.
Personen mit gemeinsamen Interessen werden hingegen, aufgrund von $r_3$, als Freunde erkannt.
Die durch $r_2$ modellierte Transitivitäteigenschaft ist ein weiterer verstärkender Faktor.
Dies lässt sich in der Clique \textit{Alice-Bob-Charlie} beobachten, die $\mathit{Friends}$-Beziehungen haben sich dort gegenseitig verstärkt.

\subsubsection{PSL~$\rightarrow$~HL-MRF Übersetzung}

Wie soeben gezeigt, ist PSL ein intuitiver Formalismus zur Definition relationaler Modelle.
Um Inferenzen durchzuführen, ist allerdings zuerst eine Übersetzung in ein HL-MRF notwendig.
Gegeben ist hierbei immer ein PSL-Programm $R = \{r_1, \dots, r_k\}$ und eine Eingabe $\mathcal{A} = \mathbb{C} \mathbin{\dot\cup} \mathbb{O}$.
Die Übersetzung erfolgt in zwei Schritten:
\begin{enumerate}
	\item \textbf{Zufallsvariablen:}
		Die Grundatome aus $\mathbb{C}$ werden zu Zufallsvariablen $X$, deren Belegung $x$ gegeben ist.
		Die Grundatome aus $\mathbb{O}$ werden zu Zufallsvariablen $Y$, für die die optimale Belegung $y \in \mathcal{Y}$ aus der Menge aller möglichen Belegungen $\mathcal{Y}$ gesucht wird.
		Der Zufallsvektor des HL-MRFs ist also $X \circ Y$, wobei mit $\circ$ die Konkatenation der Vektoren gemeint ist.
	\item \textbf{Disjunktionsklauseln:}
		In der bisherigen HL-MRF Definition wurde stets davon ausgegangen, dass die optimale Belegung \textit{aller} Zufallsvariablen gesucht ist.
		Im Falle von PSL ist allerdings bereits bekannt, dass $X = x$ gilt.
		Das Inferenzproblem lautet daher wie folgt:
		\begin{align*}
			\operatorname*{arg\,max}_{y \in \mathcal{Y}} P(Y = y \mid X = x) =&\ \operatorname*{arg\,max}_{y \in \mathcal{Y}} P(X \circ Y = x \circ y) \numberthis \\ % chktex 35
			=&\ \operatorname*{arg\,max}_{y \in \mathcal{Y}} w^\top \Phi(y, x) % chktex 35
		\end{align*}
		Um ein HL-MRF zu erhalten, muss nun $\Phi$ und $w$, d.~h.\ die Menge der Klauseln $C$ und ihre Gewichte, definiert werden.
		Die PSL-Regeln in $R$ sind zwar Klauseln, können aber nicht direkt als $C$ verwendet werden, da sie potentiell freie PSL-Variablen enthalten, die PSL-Atome also potentiell nicht in $\mathcal{A}$ sind.
		Es erfolgt daher ein sog.\ \textit{Grounding} der PSL-Regeln.
		In jede Regel aus $R$ wird dabei jede mögliche Belegung der PSL-Variablen eingesetzt, sodass die resultierende Regelinstanz nur Atome aus $\mathcal{A}$ enthält.
		Die Menge dieser Regelinstanzen wird Grundregeln (\textit{ground rules}) genannt und als Klauselmenge $C$ benutzt.
		Aus $C$ wiederum lässt sich $\Phi$ nun mittels der Łukasiewicz Logik eindeutig ableiten.
		Das Gewicht jedes Potentials $\Phi_i$ ist dabei gleich dem Gewicht der PSL-Regel aus der es resultierte.
\end{enumerate}

\subsection{Inferenzverfahren}%
\label{sec:theory:psl:inference}

In \treft{sec:theory:psl:hlmrf} wurde die Konvexität des HL-MRF-Inferenzproblems bereits diskutiert.
Prinzipiell kann daher jedes konvexe Optimierungsverfahren im Kontext von HL-MRFs verwendet werden.
In der Praxis hat sich jedoch ein Verfahren als besonders effizient erwiesen.

\paragraph{ADMM}
Das sog.\ \textit{Alternating Direction Method of Multipliers} Verfahren (ADMM)~\cite{Boyd2011} ist eine Variante des \textit{Method of Multipliers} Verfahrens, in der die dualen Variablen partiell aktualisiert werden.
Durch diese Variation lässt sich ADMM gut parallelisieren und ist somit für große Datenmengen und den Einsatz in Cluster-Umgebungen geeignet.
In~\cite[Kapitel 10]{Boyd2011} wird beschrieben, wie sich ADMM mit verteilten Datensets und verteilten Programmiermodellen wie z.~B. \textit{MapReduce}~\cite{Dean2008} oder \textit{Pregel}~\cite{Malewicz2010} implementieren lässt.

Die ADMM-Laufzeit ist proportional zur Klauselanzahl $|C|$.
Die Klauselanzahl wiederum wächst gemäß $\mathcal{O}(|\mathcal{A}|^r)$, wobei $r$ die maximale Anzahl von Nicht-Grund-Atomen in einer PSL-Regel ist.
Insgesamt wächst die Inferenzdauer also polynomiell in Abhängigkeit zur Eingabe $\mathcal{A}$.

\paragraph{BOCI}
Um effiziente Updates eines gegebenen Inferenzergebnisses für eine veränderte Eingabe $\mathcal{A'}$ zu ermöglichen, wurde das \textit{Budgeted Online Collective Inference} Verfahren (BOCI)~\cite{Pujara2015} entwickelt.
Wenn ein Inferenzergebnis für $\mathcal{A} = \mathbb{C} \mathbin{\dot\cup} \mathbb{O}$ existiert und anschließend die Wahrheitswerte für einen Teil der bislang offenen Atome aus $\mathbb{O}$ bekannt werden, muss mittels BOCI keine weitere vollständige Inferenz durchgeführt werden.

Die Grundidee dabei ist es, Metadaten, die während der letzten ADMM-Inferenz angefallen sind, für eine Bewertung jedes Atoms zu benutzen.
Die Bewertungen spiegeln die Volatilität der Wahrheitswerte der Atome in Abhängigkeit von $\mathbb{C}$ wider.
Welche Metadaten für die Bewertung verwendet werden sollten, hängt stark von der Semantik der Wahrheitswerte der PSL-Prädikate ab.
In~\cite[Kapitel 4]{Pujara2015} werden verschiedene mögliche Bewertungsverfahren beschrieben, für diese Arbeit sind jene allerdings nicht näher relevant.

Gegeben sei also eine Bewertung der Atome aus $\mathcal{A}$.
Werden nun die Wahrheitswerte bislang offener Atome aus $\mathbb{O}$ bekannt, müssen ausschließlich die $m$ höchstbewerteten Atome aus $\mathbb{C}$ neu inferiert werden, die Wahrheitswerte aller anderen Atome werden fixiert.
Je höher das sog.\ Budget $m$, desto höher ist die Qualität im Vergleich zu einer vollständigen Inferenz.
Es wurde empirisch gezeigt, dass die Inferenzqualität mit BOCI gegenüber einer vollständigen Inferenz meist nur unwesentlich schlechter ist, während sich die Inferenzdauer teils um über 60\% verringert~\cite[Kapitel 5]{Pujara2015}.
% chktex 17
